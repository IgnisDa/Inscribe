The Thinking Ladder
 September 27, 2019 By Tim Urban

This is Chapter 7 in a blog series. If you’re new to the series, visit the series home page for the full table of contents.

Notes key: Type 1 - fun notes. Fun facts, extra thoughts, or further explanation. Type 2 - less fun notes. Sources and citations.

___________

Part 3: Thinking, in 3D

“The whole problem with the world is that fools and fanatics are always so certain of themselves and wise people so full of doubts.” – Bertrand Russell

Chapter 7: The Thinking Ladder

Why do we believe what we believe?

We’ve talked about a lot of ideas in this series so far, but a common theme that runs through most of them is human belief. Our beliefs make up our perception of reality, they drive our behavior, and they shape our life stories. History happened the way it did because of what people believed in the past, and what we believe today will write the story of our future.

So it seems like an important question to ask: Why do we actually come to believe the things we end up believing?

To figure that out, we have to get good at seeing human thinking in 3D. By the time you finish Part 3, you’ll understand what I mean by that.

For now, let’s get used to seeing in 2D. That’s our mission in this chapter.

Seeing in 2D

The first dimension, as we’re defining dimensions, is the What of life. It’s what we see around us, what goes on in society, what people say and do, what they believe.

Looking at everything in one dimension just shows us what’s on the surface of all these parts of reality. But when we lift the covers off the What of life and look at what lies beneath, we’re reminded that there’s a second dimension to everything as well.

To see in 2D, we’re going to need x-ray goggles:



Without these goggles, we’d look at a human and just see this:



That’s what a human looks like in one dimension. That’s the What of a human, and it includes all of the aspects of their behavior, their beliefs, and their personality.

But when we put on our goggles, we look at the same human and see this:



Many of the human world’s mysteries become a lot less mysterious when we put on our x-ray goggles. With x-ray vision, we can see the inner psychology that lies behind the scenes of every human What.

Quick review:

As we discussed in Chapter 1, the animal world is really a world of strands of genetic code, each in a never-ending quest for immortality. Animals are just the biological containers that genes use as temporary homes during this quest. And to control their containers, genes have their animals running on primitive automated software that I’m pretty sure looks a lot like this:



The Primitive Mind in every animal—humans included—has been optimized to near perfection at getting animals to survive long enough to pass their precious genes along to new containers.

Scientists aren’t positive about the timeline, but many believe that all humans in all parts of the world lived in hunter-gatherer tribes as recently as 11,000 BC. So 13,000 years ago—or, if we call a generation 25 years, about 500 generations ago.



500 generations isn’t enough time for evolution to take a shit. So the Primitive Mind—a hardwired part of us—is still stuck in the world of 11,000 BC. Which means we’re all like computers running on the highly unimpressive Windows 11000 BC operating system, and there’s no way to do a software update.

But humans have something else going on as well—cognitive superpowers that combine together into an enhanced center of consciousness we’re calling the Higher Mind.



The Higher Mind and his magical thinking abilities helped the human species transform their typical animal hunter-gatherer world into undoubtedly the strangest of all animal habitats: an advanced civilization. The Higher Mind’s heightened awareness allows him to see the world with clear eyes, behave rationally in any environment, and adjust to changes in real time.

So while our Primitive Minds are still somewhere in 11,000 BC, our Higher Minds are living right here with us in 2019. Which is why, even though both minds are just trying to do their jobs, they’re in a fight most of the time.

Sometimes, the fights are about what’s best for us—a practical conflict.



A fight like this happens when the two minds agree on what’s important—i.e. they share a common value—but they disagree on the best way to get there. When you’re torn about whether to go for it with a Skittles binge or not, your two minds are actually in total agreement that physical health is of the utmost importance. But in 11,000 BC, where your Primitive Mind lives, there was no such thing as processed food, calories were hard to come by, and anything with a texture and taste as delectable as a Skittle was sure to be high in calories. In other words, everything the Primitive Mind knows tells it that binging on Skittles is the healthiest possible decision. Your Higher Mind, living in 2019, is extremely aware that eating 145 Skittles in a sitting is not the right health decision. And thus, your inner conflict.

The same kind of practical conflict can also lead to a lot of fights about our fears.



Sometimes, the disagreements are more fundamental.



A disagreement like this happens when two minds disagree on what’s important in the first place—a values conflict. A values conflict happens when the Higher Mind has a moral objection to something the Primitive Mind is programmed to want. Like, say, the way the Primitive Mind is programmed to try feverishly to deliver its precious genetic cargo to as large a variety of new containers as possible, at the expense of values like kindness, civility, professionalism, or marital fidelity. An inner values conflict is in progress anytime you’re coping with a moral qualm or integrity struggle.

There’s also the reverse kind of values conflict—when the Higher Mind values something that the Primitive Mind is specifically programmed to resist—like, say, pitching in with housework or donating to charity.

Humans are so complicated for a simple reason: we’re each the product of a struggle between two fundamentally different, often contradictory forces.

This ongoing struggle of fire and light is like a constant tug-of-war in our heads—a tug-of-war over our thoughts, our emotions, our values, our morals, our judgments, and our overall consciousness.

When we look through our x-ray glasses at someone whose Higher Mind is, at the moment, running the show, we see this:



With a prominent Higher Mind in total control, the Primitive Mind still makes a fuss, but the Higher Mind just yanks it back like a dog owner walking his dog.



But when the balance of power swings to the other side, the Primitive Mind is like an escaped zoo animal.





The tug-of-war isn’t binary—it’s a spectrum. Let’s call it the Psych Spectrum.



We can think of the Psych Spectrum like this:

The Primitive Mind always lives at the very bottom of the Psych Spectrum, at the base human psychological level. The Higher Mind always lives at the top, at the pinnacle of human psychological evolved potential.

Where you are on this spectrum at any given point is determined by the status of the internal struggle between the two minds.



When you’re high up on the Psych Spectrum, you get the best of all worlds. The Higher Mind understands that primitive pleasures like sex, food, and all-in-good-fun tribalism like sports fandom are awesome, and often necessary, parts of a human life. And like a good pet owner, the Higher Mind is more than happy to let the Primitive Mind have its fun. Primitive bliss is great, as long as it’s being managed by the Higher Mind, who makes sure it’s done in moderation, done for the right reasons, and no one gets hurt.

When we’re low on the Psych Spectrum, we’re short-sighted, we’re small-minded, we think and act with our pettiest emotions and our ego, we lack self-awareness, we’re short on compassion, high on hypocrisy—and because the Primitive Mind is an unconscious software program, we’re usually too foggy to see our own shittiness for what it is.

Your Psych Spectrum position ebbs and flows throughout each day, each month, and each year of our lives, as we go through happy times and hard times, great days and terrible ones, good moods and crankiness. Even a single bad night’s sleep has the potential to lower you down the spectrum the next day. But if we could quantify all of your various states, it would yield a general average. We can call that average your “psych equilibrium.” Throughout our lives, as we grow and evolve psychologically, this equilibrium can change.

Adults and Grown-Ups Blue Box

Let’s make an important language distinction: the difference between what it means to be an adult and what it means to be a grown-up. For our purposes, “adult” refers to our physical age, while “grown-up” refers to our psychological age. An adult is old. A grown-up is wise.

Each of us accumulates wisdom with life experience, but only when our Higher Mind is the one doing the thinking can we tap into that wisdom. When our willpower and our consciousness succumb to ancient software, we revert to the worst version of ourselves.

While the physical age pathway from child to adult is linear, steady, and the same for everyone, the pathway up this psychological spectrum is different for each of us. This is why we all know plenty of childish adults, as well as some surprisingly grown-up children.

Unlike our physical age, wisdom isn’t evenly distributed inside of us—we all have parts of our lives where we’re good at being grown up, and others where we tend to struggle. Our wisdom gaps—the parts of our life where we tend to be less grown-up than we are adult—are the areas where we’re psychologically stunted.



So the question isn’t if you’re psychologically stunted—the question is where?

Of course, like everything psychological, I’m sure nature plays a big role and we’re all born already prone to certain areas of psychological stunting, based on our genetic profile. But nurture is undoubtedly behind many of our Psych Spectrum struggles. I’m no psychologist, but thinking about my own mind and the minds of the people I know well, a few likely culprits come to mind:

1) Areas of life where your parents are stunted. You know how sometimes you feel like your parents are super wise about life and you admire them for it, and other times you can’t believe how unwise they can be and you feel like you have to raise them? This is because wisdom is unevenly distributed in your parents too—they’re stunted just like everybody else, going through the same internal struggle. And the thing is, areas of stunting often run in the family—you’re likely stunted in many of the same areas where your parents drive you crazy. It’s just harder to see in yourself because it might manifest a little differently, and because it tends to be harder to see our own flaws than the flaws of others.

Part of the reason stunting runs in the family is that children aren’t raised by their parents—they’re raised by their parents’ Higher Minds and by their parents’ Primitive Minds. In the areas of life where your parents tend to be wise, you were probably being raised mostly by a Higher Mind. So you learned to approach this part of life with your Higher Mind doing the thinking, and the habit usually sticks.



On the other hand, the parts of life where your parent is a baby, you were raised by…a baby. In these parts of life, your parent had yet to get ahold of their own Primitive Mind, so you were raised there by a Primitive Mind more than a Higher Mind. 



If there’s one thing a Primitive Mind knows how to do, it’s use primitive emotions like fear or pride to kick other people’s Primitive Minds into high gear. Thinking about an area of life through the lens of your Primitive Mind also becomes a habit—one that people can spend a lifetime trying to break, sometimes unsuccessfully. Wherever your parents were fearful, low integrity, petty, snobby, braggy, bratty, bigoted, jealous, cruel, entitled, uncompassionate, repressed, insecure, selfish, delusional, or any other form of foolish—those will be your psychological challenges in life. Those are the areas where you’ll be starting off from a psychologically primitive, unevolved place and where you’ll need to spend your energy and attention trying to grow.

2) Areas of life where your Primitive Mind has gone through trauma. If at any point in your past, you were bullied, neglected, shamed, ridiculed, betrayed, out-grouped, abandoned, heartbroken, discriminated against, or profoundly let down, your Primitive Mind has gone through hell. Experiences like those are burned into the Primitive Mind’s memory, and if you never fully processed what happened, the trauma will remain an open wound in your psyche. Trauma to your Primitive Mind is a wound that only the Higher Mind can tend to, disinfect, bandage up, and begin to heal. And until that happens, the previous version of yourself—the age you were when the trauma occurred—will haunt you like a ghost. Like, say, if you were socially excluded in middle school and you never properly processed that nightmare. It’ll be 20 years later and at just the hint of being out-grouped in any situation, you’ll have a meltdown and find yourself acting like a child. The traumatized middle schooler will wake up and hijack your psyche, bringing your psychological age in that moment back down to middle school. The unprocessed pains of the past are vulnerable spots in your psyche where you may be prone to fall into the unconscious hands of the Primitive Mind.

3) Areas of life where your current environment is stoking the Primitive Mind. Your current environment is made up of all the places you spend time, all the people you engage with, and the river of information that enters your brain from the external world. Your environment is constantly impressing itself upon you, and vice versa: when you speak or express yourself in any way, your environment reacts. This constant friction between you and your environment can have all kinds of effects on you—ranging from very positive to neutral to very negative. And certain parts of this relationship will inevitably stoke your Primitive Mind—a gossipy workplace, a tribalism-fueling news website, an unhealthy friendship, and plenty of other elements of our environment can regularly pull us downward on the Psych Spectrum.

Upbringing, trauma, and environment don’t perfectly cover all the bases, but my own life experience tells me that they map pretty well onto someone’s psychological still-working-on-it list.

The challenge of human growth is outlined, for each of us, by our personal list of Psych Spectrum trouble areas. This list is why life will be hard. It’s why we’ll hurt others. It’s why we have regrets. It’s what stands in between us and the life we know we should be living. And it’s a list we’ll install right into our children (if we haven’t already), unless we put in the continual self-reflection and hard work to “raise ourselves” in the areas where our parents couldn’t.

Most of the problems with humans can be boiled down to unchecked Primitive Minds getting their way against the Higher Mind’s better judgment. This is what’s behind chronic procrastination, chronic overeating, temper outbursts, infidelity, sexual assault, and all the other terrible things humans do to themselves and others. Back in the hunter-gatherer days, the Primitive Mind was mostly on point—but in the modern world, it’s our collective mental illness. And no one is immune.

Back to our dimensions. So many of our problems today stem from the oversimplification of complicated phenomena in our discussions. Thinking about the real world in one dimension is usually a bad idea.

For example, wealth in one dimension looks like this.



If you want to place blame for society’s ills, and you only have one dimension to work with, you’re left with the unnuanced option of blaming the rich, blaming the poor, blaming both, or blaming neither.

But I came across this little visual recently:



This visual is imploring us to bring one-dimensional thinking into 2D. It’s taking the wealth spectrum and expanding it (with the axes switched) into something like this:



Regardless of whether you agree with the specific viewpoint put forth in that other visual, adding a second dimension allows us to have a far more nuanced discussion about it.

For the rest of this series, we’re going to apply one particular second dimension to everything we discuss: the Psych Spectrum.

The hard thing is, we’ve all been trained to think in only one dimension—in the What dimension. We look at people’s behavior, their words, their tendencies, their habits, their demeanor, their disposition—and we stop there. The Psych Spectrum adds the critical second dimension to the equation that asks why people act the way they do. When a cashier is rude to you, you can look at the situation in one dimension, judge them to be an asshole, and stop there. Or you can put your x-ray goggles on, see that their Primitive Mind is clearly pulling the strings in their head, and wonder why. When you look at things in 2D, it doesn’t make sense to hate people who say or do shitty things, because it doesn’t make sense to hate the ancient pre-programmed survival software causing them to do so. In 2D, you see someone acting like an asshole as being “in the hands of the demon”—a demon that you know, looking at yourself in 2D, you fall into the hands of too, sometimes.

Once you get this second dimension into your thinking, you’ll notice yourself applying it everywhere. We’ve given it a try a few times before on this blog.1 Now it’s time to bring this second dimension into the world of human belief, to help us answer that key question: Why do we believe what we believe?

The Battle Over Our Beliefs

In one dimension, our beliefs landscape looks like this:



The Idea Spectrum—the same one we spent so much time with in the last two chapters—is useful when you simply want to explore what people think. This was sufficient in Part 2, when we talked broadly about the marketplace of ideas and the way a whole nation of Americans can collectively come to a conclusion, and how that conclusion can evolve over time. But as we’ll see in this post, it’s hardly a complete picture.

The Idea Spectrum is the What You Think axis. And that’s important information, but the value of what one thinks is entirely dependent upon how they came to that belief in the first place.

That’s where the second dimension comes in. When it comes to the way we think and the way we form our beliefs, the Psych Spectrum is our How You Think axis.



The addition of the Psych Spectrum turns our thinking spectrum into a thinking square—and now our discussion can get interesting.

The basic process we’re examining is belief formation, which we can notate like this:



In our thinking square, a person’s x-axis position shows us where they landed at Point B—the output of their thinking process. Their y-axis position tells us about the kind of arrow that led them there—about the process and the rationale underlying what they believe.

Where we are on this y-axis is a result of the state of our internal tug-of-war, so the first question we need to ask is: how do the two minds think?

How the Two Minds Think

In the same way our intellectual viewpoints are a function of our intellectual process, our intellectual process is a function of our intellectual motivation. We think the way we think in the first place mostly because it serves our purposes.

Our intellectual motivation will normally reflect our values—or, more accurately, the way we prioritize our values. Most of us value both health and culinary pleasure—but whether or not we binge on Skittles is determined by which of those values carries the most weight in our heads in that moment. It’s as if our values are arranged in our head as a Values Stack.



At the very top of the stack are the values we hold sacred, and as we move down the stack, we place less and less importance on each value. Whenever two values conflict with one another, the lower one will be the one to compromise, while the higher one will stand firm.

Of course, the Higher Mind and Primitive Mind totally disagree about what your Values Stack should look like.

When it comes to your intellectual life, the Higher Mind values truth above all else. The Higher Mind’s intellectual mission is to steer your beliefs—your perception of reality—as close to actual reality as he can.



This is his mission because the strength of the Higher Mind’s entire being is fed only by truth. It’s a direct correlation: the more access the Higher Mind has to truth, the brighter his light, and the wiser you are.

Given that mission, and the understanding that the mission is incredibly hard and never complete—it’s only rational for the Higher Mind to be entirely humble about his perception of reality at any given moment and totally unattached to the ideas that make up that perception. He sees beliefs as nothing more than the most recent draft of an eternal work in progress, and as he lives more and learns more, nothing makes the Higher Mind happier than a chance to revise that inevitably flawed draft. Because when beliefs are being revised, it’s a signal of progress—of becoming less ignorant, less foolish, less delusional. A change of mind about something is a good sign that his light is getting brighter—and that’s all that matters to the Higher Mind.

And how about the Primitive Mind?

It’s intuitive why the Primitive Mind would object to marital fidelity—but what’s its problem with the Higher Mind’s approach to beliefs? Isn’t truth helpful to its genetic survival mission?

Actually no, it’s not. Truth is mostly irrelevant to the Primitive Mind.

The Primitive Mind’s beliefs are typically installed into its system early on in life, kind of like the way our immune system’s settings are initially configured by our environment. The “intellectual environment” that configures our Primitive Mind’s core beliefs is typically made up of the prevailing beliefs of our family and the broader community we grow up around. On the individual level, the Primitive Mind views those beliefs as a fundamental part of its person’s identity—and therefore about as sacred as the person’s arms or lungs or heart. On the group level, beliefs are the key node that wires its person into a larger giant, which—in the Primitive Mind’s ancient world—means being safe on the lifeboat. For reasons like these, the Primitive Mind puts beliefs into the same ultra-critical category as core biological needs.

Given all of this, the last thing the Primitive Mind wants is for you to feel humble about your beliefs or interested in revising them. It wants you to treat your beliefs as sacred objects—as precious organs in your body or precious seats on a lifeboat. The Primitive Mind treats beliefs like it treats everything else—as nothing more than a means to the singular goal of genetic survival. To the Primitive Mind, the right beliefs are whatever will leave you with the strongest sense of identity and best fuse you with a large, powerful giant. An ever-evolving quest for truth is directly antithetical to these causes.

So when it comes to beliefs, the Primitive Mind doesn’t want truth, it wants confirmation—of your existing beliefs.

So where does this inner conflict leave all of us? As fucking crazy people.

To structure our little examination of our collective craziness, let’s divide the Psych Spectrum into four parts:



We can approximate each of these Psych Spectrum quartiles with a line—making our square into a Thinking Ladder with four rungs.

The Thinking Ladder is my favorite lens to use to think about thinking. For any given idea spectrum, you’ll find people on each of the four rungs. People on the same rung may disagree wildly with each other about what they think—i.e. they’ll be at different points along the rung—but what they’ll all have in common is a way of thinking, driven by a common intellectual motivation, at least when it comes to the topic at hand. Most of us are in the habit of dividing people up by what they think—by where they stand on a topic. Our goal here is to shine a spotlight on how people think and learn to categorize people that way instead.

So let’s explore each of these rungs, starting by looking at how thinking works at the top of the Psych Spectrum—where the Higher Mind is in full control—and then working our way down the rungs, until we finish up at the intellectual slums at the bottom.

Rung 1: Thinking Like a Scientist

Meet the Scientist:



This is who you are in moments when your Higher Mind is in charge of your thinking. Thinking like a Scientist has nothing to do with your line of work. It has to do with your thinking process.

We often think of science as the study of the natural world, but in the words of Carl Sagan, “Science is a way of thinking much more than it is a body of knowledge.”

Science is a way of thinking. A means of thinking. A thinking process, designed to do one thing: find truth.

When you’re high up on the Psych Spectrum, the Higher Mind’s core intellectual value—truth—will be in the top section of your Values Stack. And the Primitive Mind’s core intellectual value—confirmation of your existing beliefs—will be toward the very bottom. So it’s only logical to think like a Scientist.



It’s a good time to bring back our nail visual from Chapter 4. Just like a nation, whether we consciously realize it or not, each of us has a nail in our mind that we drive through our sacred values—those values we will not compromise on. All other values, when in conflict with the sacred values, will be forced to sway to accommodate the sacred values. When you’re thinking like a Scientist, there’s little doubt where the nail goes.



Things are simple up on the top rung: your intellectual mission is to take your existing beliefs, or lack thereof (Point A) and use your thinking process to move towards beliefs that are a little closer to the truth (Point B). You don’t know what Point B ultimately looks like, and when you’re thinking like a Scientist, you don’t care. You just want your process to take you to the truest possible Point B.



To top-rung thinkers, chasing truth is like climbing through thick fog up an infinitely high mountain. It’s the pursuit of something that can never be fully achieved, but it can be approached—and their goal is simply to continue moving up the mountain. They make their way up the mountain using their sacred process—the scientific method—as their compass. They’re intellectually flexible about everything—except the process itself.

The Scientist’s process looks something like this:



To see how it works, let’s join you in one of the areas of your life where you’re good at thinking on the top rung of the ladder, and watch you work your way through this process toward a conclusion. Starting at the very beginning—Point A.

Point A



For Scientists, Point A is almost always a loud, proud “I don’t know.” I don’t know is the top-rung thinker’s default starting place. This isn’t a righteous thing, it’s just stating the honest fact: “I don’t have knowledge about this.” When you’re honest about what you do and don’t know, knowledge and conviction are one in the same, and at Point A, you’re way down in this corner:



More often than not, Scientists don’t bother chasing truth—they know that with limited time, they’ll spend their whole lives not knowing when it comes to most topics. But when a Scientist does want to move away from A and towards B on a topic, the scientific method is the non-negotiable “due process” ideas must work their way through in order to be deemed “knowledge.”2

Let’s say that today, you’ve decided it’s one of those times when you want to learn more about some relevant societal topic.

Maybe you’re thinking about society and you want to figure out what you think about trickle-down economics, or you decide it’s time you developed an original, well-thought-out stance on abortion laws. Maybe you keep hearing about charter schools or tariffs or border policies or standardized testing and want to understand the issue better. Maybe there are local elections coming up and you want to figure out who to vote for.

Maybe you’re thinking about history and you want to get a better understanding of the causes of World War I, or maybe you’ve been reading the news and you want to know why Brexit happened and what it means for the future. Maybe you’re just wondering how scared or excited you should be about the technological explosion happening all around you.

Whatever the topic, you’re ready to embark up the high-rung thinker’s foggy mountain in search of a truer Point B—so you get going on your first goal:

Forming a Hypothesis

Top-rung thinkers form hypotheses from the bottom up, by reasoning from first principles. When you reason from first principles, you do your best to ignore conventional wisdom and your own preconceptions, and you focus only on fundamental facts. You treat those core facts—the “first principles”—like puzzle pieces, and using only those pieces, you employ rationality to puzzle together a conclusion.

But in order to puzzle, you need pieces, and at Point A, you don’t even have those yet. Getting the right pieces and eventually assembling them into a hypothesis is a three-part effort, carried out by these three characters:



Here’s how they do their thing:

Forming a Hypothesis, Component 1: Gathering Information



When we think about how you form your viewpoints, we can think of your head as a two-region system, with a gate surrounding each region:



The Attention Bouncer’s job is to guard the attention gate. Each of us is flooded with information at all times, and we have severely limited attention to allot. In other words, your mind is an extremely exclusive VIP-only club. As you scan the world around you, pieces of information form a long line outside your Attention Gate, and your Attention Bouncer has no choice but to be a real dick of a bouncer and turn away almost everyone.



The exception is when you decide you want to learn something new and develop a viewpoint on it. When you’re thinking like a Scientist, you know how little you know, especially when you’re at Point A. So the Attention Bouncer keeps the filter very loose for any info that seems potentially relevant to the topic.

As he mans the Attention Gate, the Attention Bouncer isn’t especially concerned with whether the puzzle pieces are reasonable or even accurate—that’s above his pay grade. He just wants to import a wide variety of pieces that seem to be representative of the full range of ideas out there on the topic at hand, from all across the idea spectrum. He knows that even a viewpoint you’re pretty sure you disagree with, from a person you’re pretty sure you don’t like, can teach you something. It may not end up changing your mind, but learning about the way that person thinks is information in itself. In this stage, even viewpoints you know are ridiculous are allowed in if they seem to be prominently represented, because the range of viewpoints that exist about the topic is a key facet of understanding the topic.

Soon, the outer region of your mind—the “considerations region”—is jam-packed with eager puzzle pieces, standing in a long line of their own outside a far more exclusive VIP club—your beliefs.

Forming a Hypothesis, Component 2: Evaluating Information



There are instances when a thinker has the time and the means to collect information and evidence directly—with their own primary observations, or by conducting their own studies. In these cases, the Belief Bouncer doesn’t have much to do—you already know that the information is reliable, because you saw it with your own eyes.

But the vast majority of the info we use to inform ourselves is indirect knowledge—knowledge accumulated by others that we import into our minds and adopt as our own. Every statistic you come across, every study you learn about, everything you read in a textbook, everything you learn in school, everything you learn from your parents, every book you read, everything you see or read in the news, everything you read on social media, everything you hear a politician or celebrity say, every assumption of conventional wisdom—it’s all indirect information.

Your Belief Bouncer’s job is to decide what’s true and what’s not.



Assessing an idea’s legitimacy is hard work. And if the only means of assessment is to verify the truth of it yourself, you’re not saving much time.

That’s why perhaps the most important skill of a savvy thinker is learning how to dole out trust.

Trust, when assigned wisely, is an efficient knowledge-acquisition trick. If you’re able to trust someone who actually speaks the truth, and you accept what they say as truth, you’ve taken the knowledge they worked hard for—either through their own primary research or indirectly, using their own carefully designed trust criteria—and essentially photocopied it into your own brain.

Without trust, knowledge is limited to a tiny dataset of personal experience. Raise infant Einstein in the forest with no information and then tell him to invent the best weapon he can, and he won’t get anywhere near the bow and arrow. But start Einstein off standing atop a knowledge skyscraper and he can outline how general relativity works. Communication, plus trust, is a magical intellectual corner-cutting tool—and it’s the reason humanity has been able to accumulate a skyscraper of collective knowledge over the past 10,000 years that has led a species of forest primates to understand the origins of the universe.

But trust, when applied wrongly, does the exact opposite. When we trust information to be true that isn’t, we end up with the illusion of knowledge—which is far worse than having no knowledge at all.

The fact is, almost all the learning you’ve done in your life has flowed into you through a trust channel presided over by your Belief Bouncer. But what matters far more than the quantity of your cumulative learning is the quality—the legitimacy—of what you’ve learned. And the quality of your knowledge is a function of how good your Belief Bouncer is at assigning trust.

In a way, learning is kind of like running a business and knowledge is like the cash you collect. Accepting bad information as truth is like accepting a customer’s payment in Monopoly money without realizing it. You can build a great business and work hard for decades running it, but if 90% of your income has been counterfeit Monopoly money, you’re still a terrible businessperson.

It’s easy to see that Monopoly money is fake, but Monopoly knowledge often looks exactly like real knowledge to the untrained eye, so our knowledge economy is currently infested with counterfeit information.

To do his critically important job, your Belief Bouncer needs to master the art of skepticism.

The Skepticism Spectrum

The truth-seeker’s goal is to hone in on a skepticism sweet spot—something we can visualize on a Skepticism Spectrum.



The Skepticism Spectrum is a filter whose settings can be adjusted to make the filter stricter or looser. The sweet spot is the filter’s optimal setting—just tight enough to consistently identify and weed out bullshit; just open enough to let in the truth and take full advantage of the magical corner-cutting of adopting the real knowledge of others as your own.

Move to the left of the sweet spot on the spectrum, where the filter is looser, and you begin to allow some bullshit to flow into your beliefs. Move even farther left and your beliefs become packed with a jumble of falsehoods, misconceptions, and contradictions. Moving to the right of the sweet spot tightens the filter too much, like a business owner so worried about accidentally accepting Monopoly money that they end up turning down real money too. Go way to the right and you’ll stop believing the moon landing happened.

To be a healthy eater, you need to find the food intake sweet spot—too lenient a “Food Bouncer” and you eat a ton of junk food, too strict a bouncer and you starve. Being a healthy thinker works the same way. Both gullibility and paranoia, especially as you move farther away from the sweet spot, cripple your ability to be a healthy thinker. When you’re being gullible, contradictions become a nightmare for you. You read Opinion A on a topic and you’re sold, adopting it into your beliefs. Then you read Opinion B, which says Opinion A is wrong, and you’re sold on that. With wildly contradicting viewpoints jostling over your beliefs, you end up withdrawing from the topic, feeling like you’re just not capable of understanding it. On the other side, when you’re being paranoid, everything you hear comes along with a little asterisk that says, “don’t be a chump—they’re probably trying to manipulate you.” You refrain from adding almost anything to your knowledge bank. In both cases, learning stalls.

Both gullibility and paranoia are a sign of a lack of confidence in your own judgment of who and what is trustworthy. The key skill that’s missing is the ability to accurately separate the real from the skewed from the misleading from the totally incorrect. If you can’t judge incoming information, you can’t take advantage of the critical tool of indirect knowledge.

The Rung Rating: A Trust Shortcut

Effective thinkers, usually without consciously realizing it, develop an internal trust rating system for friends, journalists, politicians, media brands, institutions, and any other source of information. Like an intellectual credit score.

We’ll call this credit score a Rung Rating—because what they’re really evaluating is which rung of our Thinking Ladder the source is “thinking from.”

A good trust rating system adds even more efficiency to the knowledge acquisition process, saving the Belief Bouncer the heavy lifting of having to evaluate every piece of incoming info on its own merits. We have an internal trust rating system in our heads for the food we eat. If you trust a certain grocery store or food brand or restaurant, you can save the time of carefully scanning ingredients before every bite. The Rung Rating works the same way.

On top of being an efficient time-saver for acquiring info from trusted sources, it’s a trick that allows you to gain real knowledge from less-than-healthy information as well. If I know that a particular source, on Topic X-Y, tends to be strongly pro-X and a bit biased against Y, but they also have shown to be reasonably concerned with truth, I can accept their views about X or Y as real information, as long as they’re taken with the proper grain of salt. Depending on the magnitude and direction of their bias, I can apply just the right size and flavor salt grain to their views. If they say something good about X, I can consider that point but know I need to verify it myself, since their historical bias in favor of X means this is just a default statement for them. Same for a negative statement about Y. On the other hand, if they say something good about Y, their historically anti-Y bias makes this very meaningful information that I can probably accept right off the bat.

To get anywhere as a thinker, you need to start with the right puzzle pieces. The Attention Bouncer and Belief Bouncer, through a team effort, are in charge of getting you those pieces. If they’re doing their job, the super-VIP club of your beliefs ends up populated by a large array of useful information—and you’re ready to get puzzling.

Forming a Hypothesis, Component 3: Puzzling Together a Hypothesis



In the center of the beliefs region of your mind, your Puzzler starts putting pieces of information together.



Your Puzzler knows that the two bouncers guarding his laboratory have done a ton of listening to others, and his job is to now block out all of that noise and build his puzzle using nothing other than rationality and the info already in your head.

When you reason from first principles, you’ll often come to conclusions that conflict with your pre-existing beliefs or with conventional wisdom. It’s a challenge not to lose trust in yourself and give up on your thought process in these moments. But an experienced high-rung thinker knows how often even consensus beliefs are wrong, and their Puzzler will forge ahead confidently, knowing that in a world full of dogma and misinformation, his diligent, honest reasoning process is as valid a path to truth as any.

Scientists, so rigid about their high-up position on the How You Think axis, start out totally agnostic about their horizontal position on the What You Think axis. Early on in the puzzling process, a Scientist treats their rung like a skating rink, happily gliding back and forth and flip-flopping their opinion as they explore different possible viewpoints.



But as the Scientist learns and starts to puzzle info together, they increasingly hone in on a portion of the Idea Spectrum that they suspect may be where the truth lies. Their puzzle is finally taking shape—they have begun to form a hypothesis.

This is an exciting moment on your mission today—you’ve conquered the first section of the scientific method. You’ve learned a ton and you’ve formed an authentic take on the topic.

Now comes the unpleasant part.

Testing the Hypothesis



Imagine I present to you this boxer.



And I tell you



You might ask



If I answered



You’d probably ask



And if I answered



You’d think I was insane.

But people do this with their ideas all the time. They feel sure they’re right about an opinion they’ve never had to defend—an opinion that has never stepped into a ring.

A belief or opinion you haven’t tested isn’t knowledge, it’s wannabe knowledge—i.e. a hypothesis. A hypothesis is a boxer with potential—but it’s not a champion of anything.

In the world of ideas, the marketplace of ideas is the boxing ring, and boxing opponents come in the form of dissent. 

When you’re thinking like a Scientist, you know that there are major barriers between you and truth. No matter how fine-tuned your Rung Rating filter system is, it’s sure to mess up sometimes and let falsehood toxins find their way into your hypothesis. You’re also wary of your own biases and the inevitable stubbornness of your own worldview. This is why dissent is so critical for every Scientist. Dissent is the truth-seeker’s immune system. If your hypothesis is a machine, things like biased reasoning, oversimplification, and misleading statistics are its glitches and bugs, and a feisty dissenter to the hypothesis is like a technician searching for those flaws, which helps make it a better machine. Stuart Mill says it best:1

There is the greatest difference between presuming an opinion to be true, because, with every opportunity for contesting it, it has not been refuted, and assuming its truth for the purpose of not permitting its refutation.3

That’s why a Scientist views dissent as another valuable puzzle piece, critical to the completion of their understanding.



So you take your hypothesis out of the safety of your head—



—and you toss it into the gauntlet of the marketplace of ideas. You start expressing the idea publicly, in person and online, and watch it get pelted by criticism from all angles. It’s time to see if the little guy can box.



High-rung thinkers tend to surround themselves with other high-rung thinkers—which means finding people to debate with will be no problem. When high-rung thinkers hear an idea, they reflexively look for holes to poke in it. They disagree with each other for sport.

So you just put the idea out there and watch people flock over to try to break it.



Sometimes they succeed.



Other times your idea holds strong.



Watching your idea in the ring exposes flaws or misconceptions or soft spots in your ideas and helps you see where to make adjustments. Sometimes you engage in debate, playing the role of defender of your idea, arguing for its validity as hard as you can.



Sometimes you engage in dialectic, joining the dissenter in examining your idea.



You sometimes even try flipping sides and playing devil’s advocate, finding someone who agrees with you to debate with since looking at your conclusions through another lens enhances your clarity and reveals things you missed.



As your hypothesis boxes different dissenting ideas, you keep your salt shaker handy, viewing every boxing match through the lens of what you believe to be the dissenter’s Rung Rating. You also take the dissenter’s degree of conviction into account, engaging in what writer Julia Galef describes as “meta-updating”: if you’re arguing with someone who has previously shown that their conviction tends to carry integrity, and you’re both pretty sure you’re right in the argument, you assess which of you seems more sure they’re right and use that as an important piece of information.



On the other hand, when you argue with someone who has shown you in the past that their expressed conviction is often dishonest or delusional, boy-who-cried-wolf style, you don’t view their level of certainty as meaningful information.



The more boxing matches you put your hypothesis through, the more you’re able to explore the edges of your conclusions and tweak your thoughts into crisper and more confident opinions. Your hypothesis is starting to get some serious gauntlet cred.

The gauntlet isn’t kind to most ideas, because most ideas are flawed. And when your hypothesis is flawed, some time in the gauntlet watching it get its ass kicked will lower the conviction you feel and convince you that you need to go back to the drawing board. But the times when you come up with a hypothesis that’s logically sound, thoroughly fact-based, and philosophically consistent, the marketplace of ideas will have the opposite effect: watching your idea box like a champ again and again will make you feel tremendously confident about your opinion.

As you work your way through this process, we come across yet another important thinker skill—keeping the appropriate level of humility as your confidence grows.

You know how I said there’s a “skepticism sweet spot”—right in between gullibility and paranoia—where your skepticism is enhancing and not hindering your ability to learn? Well the same thing goes for humility.

That sweet spot falls along this dotted line:



At any given point on your knowledge quest, you want your conviction to be an accurate expression of how much you actually know.

Even for the most self-aware thinkers, staying on the diagonal is easier said than done. It’s like walking on a tightrope, and it’s easy to fall off.

When you fall off the tightrope to the lower direction, you end up in the insecure zone.



The insecure zone happens when you forget a critical fact: that most people expressing conviction are full of shit. When you forget that, but you’re just self-aware enough to not feel the same BS conviction yourself, you feel stupid, and you feel ashamed of your more modest levels of knowledge.

When you’re gullible to other people’s conviction—like a customer who falls prey to a snake oil salesman—your conviction drops below your own knowledge level. You actually do know some stuff about the topic, but you feel like you don’t. You have a confidence problem. The farther below the tightrope you are, the bigger the problem.

When you fall off the tightrope to the other side, you land in the arrogant zone.

 



The arrogant zone happens when you’re not self-aware enough to remember your own flaws, and you forget how hard knowledge is, and you fool yourself into believing you know more than you do. The higher up you are above the tightrope, the fuller you are of shit.



But so far on our little journey, you’ve been doing things like a Scientist. When you’re thinking like a Scientist, it means the Higher Mind is strong in your head, and it lights you up with clarity and self-awareness. High self-awareness means that when you look in the mirror, you see yourself as you are—a flawed thinker with endless potential to learn.



Clear self-awareness helps you keep an eye on the Primitive Mind and stay aware of your own cognitive pitfalls—because you know that your brain was designed for survival, not truth, and it’s wise to be wary of your own intuition. As you learn and ponder, self-awareness helps you resist your brain’s urge to save mental energy and oversimplify a complex topic. It helps you remember to think in spectrums despite an instinct to think in binary black-and-white. It helps you force yourself to parse slightly different versions of similar ideas instead of more conveniently grouping ideas together and labeling them as a single thing. It helps you keep putting in the effort to search for the nuance. Because a Scientist knows that the truth is always buried somewhere in the wrinkles of nuance, and that a satisfying, clean-cut, one-sided viewpoint is almost always wrong or incomplete. Looking in the mirror helps you keep an eye on your own Rung Rating as you do your thinking.

So you manage to stay pretty close to the tightrope, working your way somewhere up here:



Your little hypothesis has gone through hell and come out on the other side as knowledge.

 

There’s just one more thing to do.



Top-rung thinkers know themselves, their peers, and history well enough to know that all human intellect is fallible. In the actual science world, even the most tried and true hypothesis will be treated not as ultimate truth, but as a theory. Scientists max out at “theory” because they know that all beliefs are falsifiable and subject to be proven wrong by changing times or new evidence. Thinking works the same way—in order to continually claim a hypothesis to be “knowledge,” it must survive continued testing and scrutiny.

___________

If all of this sounds a bit exhausting, that’s because it is.

The Scientist’s learning process is exhausting because knowledge is hard. Because truth is hard. Which is why people who tend to think like Scientists are more than happy to say “I don’t know” most of the time. They’re lazy like anyone else—the thing that makes them Scientists isn’t that they’re necessarily obsessive learners, but that they’re realistic about what developing real, independent, informed viewpoints entails, and they’re honest with themselves and others about what they know and what they don’t.

But for today at least, you’ve gone hard enough at this to establish a solid, independent viewpoint that you can feel confident about—and you’ve done it the Scientist way.

___________

I recently attended a conference for scientists (actual, science scientists). I’m not a scientist, but I’m a curious person, and I often write about science, so I had a delightful time spending the whole conference cornering scientists and grilling them on questions I’ve had but haven’t been able to answer.4567 I also often write about how scientists think, and at one point, standing in a circle with about four scientists, I talked about how much I admire the way scientists think—their humility, their pure motivations, their willingness to admit they’re wrong when new evidence changes their conclusions. 

They all burst out laughing. One of them said, “Have you met scientists?” 

This is the thing about humans. We’re so bad at thinking like Scientists that even scientists are bad at thinking like Scientists. Because no matter who you are, inside your mind is a powerful little primitive fuzzball.

Above, when you were thinking like a Scientist, the conditions in your head were pristine. The Higher Mind was doing the thinking while the Primitive Mind lay mostly dormant. We’re all there sometimes, when we’re being our best selves—when we’re thinking like pros.

But truth is a fragile motive. And at some point, without you realizing it, something changes. It’s a bit of a chicken and egg situation. Sometimes, a topic you’re thinking about jolts your Primitive Mind awake because it believes, for some reason, that holding a certain viewpoint on this topic is important for your survival—often because the topic has become tangled up with your identity. Sometimes, the Primitive Mind has activated for some unrelated reason, which infiltrates your mind with one or more of its standard emotions—fear, pride, anger, ego—and this then affects your thinking, bringing your intellect down closer to its level.

Whatever the cause, on our journey down the ladder today, the Primitive Mind has entered the picture. The Higher Mind is still the more powerful character, but now he’s got competition.



The Primitive Mind’s involvement has compromised your ability to think from the very top rung—you’ve dropped down to Rung 2. You’re no longer thinking like a Scientist—you’re thinking like a Sports Fan.

Rung 2: Thinking Like a Sports Fan

The biggest psychos aside, most real-life sports fans want the games they watch to be played fairly. They don’t want corrupt referees, even if it helps their team win. They want their team to win fair and square. They place immense value on the integrity of the process itself.

It’s just…that they really really want that process to yield a certain outcome. They’re not just watching the game—they’re rooting.

When your Primitive Mind begins to infiltrate your reasoning process, you start thinking the same way. When you head down this road—



—you still believe you’re starting at Point A, and you know you need to work hard to get to Point B—and you do want the Point B you ultimately arrive at to be the truth. But you’re not exactly objective about it.



Weird things happen to your thinking when the pure drive for truth is infected by some ulterior motive. Psychologists call it “motivated reasoning.” I like to think of as Reasoning While Motivated—the thinking equivalent of drunk driving. Sent-ts’an explains:

If you want the truth to stand clear before you, never be for or against. The struggle between “for” and “against” is the mind’s worst disease. – Sent-ts’an, c. 700 AD

When you’re thinking like a Sports Fan, Sent-ts’an and his apostrophe and his hyphen are all mad at you, because they know what they’re about to see—the Scientist’s rigorous due process of thinking, corrupted by the truth-seeker’s most treacherous obstacle:

Confirmation bias.

Confirmation bias, one of the most common impairments caused by Reasoning While Motivated, is why people see perfection in their brand new romantic relationships, and why people are so often nauseated by exes they once adored. It’s why socially paranoid people freak out when someone takes too long to respond to their email. It’s why overconfident artists can hear ten lukewarm reactions to their art and one effusive one and see the single exception as evidence of their greatness—and why other insecure artists do the exact opposite. It’s why conspiracy theorists see evidence of their conspiracies everywhere.

Our x-ray goggles remind us what’s going on here: While the Higher Mind is all about the How You Think axis—about the integrity of the process of thinking—the Primitive Mind is the opposite: it cares only about what you think—about your x-axis position. Because the two minds’ intellectual goals—truth and confirmation—are in direct conflict, it’s a zero-sum situation. When the Primitive Mind enters the equation and captures a piece of your mindset, it inherently draws some of your integrity away from the Higher Mind’s principles. Confirmation rising up in your Values Stack also means truth moving down.

Truth being lowered a bit in your Values Stack moves it out of sacred, nailed-in territory and into the important-but-not-totally-sacred area, where values are fastened in a less rock-solid way—let’s say, with a thumbtack.



At the same time, confirmation has moved up from the unimportant area of your Values Stack—where values swing freely, totally at the whim of the more important values above—to the somewhat important tier. Values in this tier are still secondary to the more important values, but they’re given enough weight that they don’t swing so freely anymore. They’re taped in place now.



With some effort, taped values can be moved, but there’s some friction now—a resistance to changing your mind that wasn’t there when you were on the top rung. A portion of your intellectual integrity has been supplanted by intellectual loyalty.

Life is simple for you when you’re dealing with only nailed-in and free-swinging values—there’s little inner conflict. Thumbtacks and tape are trickier.

Now, as the Higher Mind tries to chase truth, confirmation bias is the invisible hand of the Primitive Mind that nudges the process in a preferred direction. And this invisible hand infects every part of the knowledge-acquisition process.

The Sports Fan’s Thinking Process

If you’re not looking closely enough, the Sports Fan’s thinking process looks a lot like the Scientist’s:



But as we watch you work your way from A to B, we’ll see that a little Primitive Mind can go a long way.

From the very start, when you’re standing at Point A, without any actual knowledge yet, the influence of your Primitive Mind’s automatic conviction has you feeling like you know a bit more than you do.



The gathering evidence phase—which the Scientist made sure to do in an even-handed and representative manner—now becomes motivated gathering. The Attention Bouncer now plays favorites.

We all know what it’s like when a bouncer plays favorites. Back in my 20s, I suffered through a reasonable amount of hellish nightlife—and a common experience was waiting in line outside some dark, loud, nightmarish weekend night bar while groups of women would walk up and gain immediate entry, without waiting in line. If I were with a group that was too guy heavy, we might get all the way to the front only to not be let in at all.

When you’re thinking like a Sports Fan, your Attention Bouncer treats information that concurs with your existing opinion like a group of young, attractive women. And he treats info that weakens your existing views like a large, unappealing group of bros. Wary of letting your mind turn into a sausage fest, he plays favorites.

When favoritism happens in the realm of ideas, we call it cherry-picking.

According to the internet, the origin of the term has to do with actual cherry-picking. Imagine you’re a very rich person who owns a huge estate, and one day you’re bored as shit so you call your servant into the room and tell him you’d like to get a sense of what the cherry harvest is like this year in your orchard. But you’re also hungry, so you order the servant not to tell you how the cherries are looking but to actually bring you a representative sampling of the cherries in your orchard.

If you were thinking on the Scientist rung of the ladder, you’d leave the instructions at that. Your servant would count the cherries in the orchard and collect a sampling like this:



But if you were thinking from the Sports Fan’s rung, you might add in one extra comment while issuing the command: “…and it will upset me greatly if my cherries aren’t having a good year. I look forward to learning how well they’re doing.”

So now, the servant heads out to the orchard with the same plan—to pick a representative sampling. But with your last comment ringing in his ears, he finds himself not really counting the cherries but more eyeballing them. And when he picks his sampling, it comes out like this:



As a Rung 2 thinker, you still want to know what’s actually happening out in the world—you’re just nudging the results a little. And that’s what you did here in the info gathering phase, with your Attention Bouncer as the servant. You didn’t exactly tell the bouncer to be dishonest about anything—you just put him in an awkward position by providing conflicting motivations: 1) to give you a representative sampling, and 2) to hope that the results come out a certain way. So the info you end up gathering and absorbing on the topic skews a little friendly to your preferred conclusion.

As this happens, you’re not conscious of doing anything weird at all. You still believe you’re thinking like a good Scientist—even if somewhere very deep down, you might feel a little worse about yourself without being quite sure why.

And as we move on from info gathering to info assessment, the trouble continues—because when you’re thinking like a Sports Fan, the Belief Bouncer starts acting funny too.

When presented with an idea that confirms your existing beliefs, he becomes more lax with the door, easing up on the trust filter criteria.



But when a piece of evidence doesn’t seem to match your Primitive Mind’s favorite idea, that benefit of the doubt vanishes, and the Belief Bouncer looks for any reason he can find to deny entry.



We talked about the problem of being either too gullible or too paranoid—but there, we were talking about genuine truth-seekers who were simply insecure about their intellectual judgment. Toggling back and forth on the Skepticism Meter, based on the content being evaluated, is motivated skepticism—a classic form of Reasoning While Motivated.

Social psychologist and NYU professor Jonathan Haidt sums up motivated skepticism nicely:2 

We don’t look out at the world and say, “Where’s the weight of the evidence?” We start with an original supposition and we say, “Can I believe it?” If I want to believe something, I ask: Can I believe it? Can I find the justification? But if I don’t want to believe it, I say: Must I believe it? Am I forced to believe it? Or can I escape?

“Can I believe it?” isn’t accidental gullibility—it’s motivated gullibility. Likewise, “must I believe it?” is motivated paranoia.



Haidt goes on to reference a perfect example of motivated skepticism in action:

In a classic study, students come into the lab. They’re taking psychology classes, they’re learning about experimental methods, so they’re given a study. It looks like it’s from the Journal of Science. They’re asked to critique the methods. And the study seems to show that caffeine consumption is associated with breast cancer. And their job now is to read the study and say what they think of the methods.

Well, who do you think finds a lot of flaws in that study? Who do you think? Coffee drinkers! And do you think all coffee drinkers are trying to find flaws in the study? Women who drink coffee are desperately saying, “Must I believe it? Must I believe it? What could possibly be wrong,” and they find all kinds of things wrong with it. The others say, “Oh gosh, okay, I didn’t know that.”

Reasoning While Motivated caused the exact people who were most upset about the study’s results to reason differently, and less accurately, than the rest of the participants—because it literally changes what goes on in our brain. An MIT study used actual fMRI data to see what was going on with motivated reasoning, and they found that “motivated reasoning is qualitatively distinct from reasoning when people do not have a strong emotional stake in the conclusions reached.”3 Kinda like how drunk driving is qualitatively distinct from sober driving.

We could also return to the story of smoking in the US that we discussed in Part 2 and see the same exact phenomenon. One thing I didn’t mention is that as the country slowly came around to the reality that smoking is bad for your health, no one came around more slowly than smokers.4

When I see these stats, I see this story.



Smokers were fans of the “smoking is fine” sports team—which impaired their ability to reach truth as quickly as those without a dog in the fight.

But neither the smokers nor the female coffee drinkers were thinking about any of this. Like a drunk driver who’s pretty sure they’re fine to drive, they assumed they were being objective. Because part of motivated reasoning’s sneakiness is that the thinker doesn’t realize that it’s happening.

The same bias alters the way you judge info sources. It works kind of like adjacent trains. If you’re on a train looking out the window at another train and both trains are still, you see the other train as still.



If the other train then starts moving forwards at 10mph, it appears to you as it really is—moving forwards at 10mph.

But if your train is moving forwards at 10mph, then the other train, also moving forward at 10mph, looks to you like it’s stationary.



And if the other train then stops moving and becomes stationary, it looks to you as if it started moving backward at 10mph.



Humans with bias works like trains in motion. When you’re thinking clearly and objectively, like a Scientist, you’re like a stationary train. You see objectivity and bias in other thinkers for what they are. But when you’re seeing through a biased lens, you’re like a train in motion. Someone who shares your bias seems objective to you, while someone being objective seems to you to be biased the other way. And when you come across someone who is actually biased the other way, you’ll see their bias as more extreme than it is.

The motion of your own bias skews your Rung Rating system. Instead of judging thinkers or info sources purely by the strength of their intellectual process, you’ll unconsciously inflate the rating of sources who agree with you and dock your rating of those who don’t. More motivated reasoning—this time in the form of motivated judgment.

This Rung Rating distortion works hand-in-hand with the Skepticism Meter toggling.



A corrupt Attention Bouncer leads to a distorted picture of reality, and a corrupt Belief Bouncer means a weakened intellectual immune system against toxic falsehoods. So when we head onto the third leg of the “hypothesis formation” process—puzzling together a hypothesis—we find a room of beliefs curated by a corrupt process. The key character in that room—the Puzzler—is limited in his puzzling to the pieces he’s given, and when you’re thinking like a Sports Fan, a disproportionate number of those pieces will support your existing views.

And that’s only half the problem, because the Primitive Mind’s motivation has infiltrated the process of all parts of your mind—including the Puzzler himself. As he puzzles, hovering somewhere in his peripheral vision is a picture of your existing viewpoint—the one your Primitive Mind wants so badly to confirm—and the Puzzler is a bit more inclined to use the pieces that match the picture.



By the time you arrive at your eventual hypothesis, it looks conveniently and predictably similar to what you suspected/hoped was true back when you were at Point A.



When you move onto the final step in the knowledge process—testing your hypothesis—your Sports Fan bias continues to rear its head.

When you were thinking like a Scientist, you felt very little attachment to your hypothesis. But now, as you watch your little machine box, you’re watching as a fan. You’re wearing its jersey. It’s Your Guy in the ring. And if it wins an argument, you might even catch yourself thinking “we won!”—a classic term in the world of real sports fans that reveals their identity’s entanglement with their team.

You’re acting like a weirdo because when you’re thinking like a Sports Fan, you don’t see dissent like a helpful puzzle piece—you see it like a tennis ball coming at you during a tennis match. Something to hit back—a challenge to try to defeat.



And as you analyze what takes place during the boxing match, there’s more motivated judgment. When a good punch is landed on your hypothesis, you’re likely to see it as a cheap shot or a lucky swing or something else that’s not really legit. And when your hypothesis lands a punch, you may have a tendency to overrate the magnitude of the blow or the high level of skill it involved. When the match is over, you usually end up feeling like your hypothesis passed the test with flying colors—even in cases when an objective observer would see it the opposite way.

At the end of all of this, it’s no surprise when you end up right at that shiny green Point B.



Your road to Point B was easier this time. And even though you learned a little less to get there than you did when you were a Scientist, you feel a little more confident about your beliefs than you did then.



Your Primitive Mind’s suite of confirmation bias tricks has brought you up above the humility sweet spot, into the Arrogant Zone. Even though in reality, you’ve diminished as an effective thinker, when you look in the mirror you see a better thinker than the Scientist saw through their mirror.



As I said, a little Primitive Mind can have a big impact.

But Sports Fans aren’t hopeless. The Higher Mind still has the edge in the Sports Fan’s mind, which is why the Sports Fan, motivated as they are, still goes to such great pains to try to go through all the steps of the thinking process. The Sports Fan gets that the scientific method is incredibly important—they’re just not great at working through it effectively.

But the Higher Mind will be a nagging voice of self-doubt in the Sports Fan’s head, and at least some of the time, the Sports Fan can reluctantly acknowledge that they’re wrong. Watching their favorite team play, a real-life sports fan’s eye may be biased, but when a slow-motion replay clearly shows that the opposing team was in bounds and got both feet down, they will grudgingly concede that it was the right call—in the end, if the dissent is strong enough, the Sports Fan’s views are falsifiable. Deep down, when push comes to shove, the integrity of the game matters most to the Sports Fan—because underneath all the haze of cognitive bias, Sports Fans are still real thinkers.

This is why a lot of Sports Fans end up on a Dunning-Kruger-type path, where the Higher Mind eventually prevails and the Sports Fan starts thinking more like a Scientist.



When the scientists I talked to at that conference laughed and asked me, “Have you met scientists?” I think they were probably referring to the fact that scientists, like almost all good thinkers, often drift down a rung and think like Sports Fans. Sports Fans are imperfect Scientists—Scientists who have, at least in a particular moment or on a particular topic, fallen off the thinking wagon and let their Primitive Mind get the best of them. The key is that when Rung 1 thinkers go biased, they usually go Rung-2 biased—but probably not much lower.

___________

As we move down the ladder from here, let’s remind ourselves that even though we’re using distinct rungs to simplify things, we’re really working on a spectrum.

When you’re just below the Scientist rung, you’re just being a little bit of a Sports Fan. Yes, you’ve begun Reasoning While Motivated, but you’ve only had a couple drinks at the motivation bar and your Higher Mind is still in near-total control.

But as you drift down the Psych Spectrum, the influence of the Primitive Mind becomes more and more prominent. Your feet get stickier on the x-axis, as it becomes increasingly difficult for even the strongest dissenting evidence to move your beliefs. As the smoke clouding your reason gets thicker, your self-awareness dulls. Your Blood Motivated Level rises higher and higher as truth and confirmation are driven closer to each other in your Values Stack.

Eventually, you cross the mid-point.



This is a big moment. Because now, the Primitive Mind is the more powerful character in your mind.

 



And when the Primitive Mind becomes the alpha character, confirmation becomes more important to you than truth.



Whether you’ll admit it or not (you won’t), the desire to feel right, and appear right, has overcome your desire to be right. And when some other motivation overcomes your drive for truth, you leave the world of integrity, of rationality, of reality, and enter a new place—a place I call:



Unfalsifiable Land is a great world of green grass, blue sky, and a bunch of people whose beliefs are unable to be swayed by any amount of evidence. When people are here, they believe what they believe not because of independent reason, but because they are disciples of some line of thinking—that of a religion, a political ideology, a subculture—or maybe they’re simply clinging onto the conclusions of their previous self, back when that self was more of a real thinker. Either way, you can argue with them all you want, but you will achieve nothing, because their views are not falsifiable. That’s why they live here.

Even though the vertical axis is a smooth spectrum, the mid-line is a key point along it. Having crossed it, What You Think is now more important to you than How You Think.

When your thinking descends from Rung 2 to Rung 3, you’ve gone from a Sports Fan to a different kind of thinker entirely.

Rung 3: Thinking Like an Attorney

An Attorney and a Sports Fan have a lot in common. They both have a preferred Point B, while also still maintaining some level of dedication to the arrow that’s supposed to take them there. They’re both conflicted between the values of truth and confirmation. The critical difference is in which value, deep down, is higher in their Values Stack.

A Sports Fan wants to win, but when pushed, they care even more about fair play than winning.

An Attorney’s job is to win, and no matter how hard you push them, nothing can alter their allegiance. Because has this ever happened?



No. That has never happened.

Because an Attorney is on a team, period. 

Which means that while the Sports Fan starts at Point A and then kind of tries to nudge the arrow in a certain direction, when you’re thinking like an Attorney, you don’t start at Point A at all.

You start at Point B.



When you’re thinking like an Attorney, the faint voice of the Higher Mind in your head means you’ll still feel the need for there to be an arrow that leads to Your Idea—but instead of Point B being the dependent variable to the arrow’s independent variable, your process renders the arrow at the whim of Point B. You’ll put your effort towards piecing together an arrow that leads right where you want it to.



This is how attorneys in the real world think, isn’t it? They decide to take on a case, or they’re assigned to one, and from the first moment they’re thinking about the case, they already know their overall stance—and this is where they’ll stay, regardless of what their reasoning or the evidence says.

The client is not guilty. Now let’s figure out why.

From there they go through their due diligence, cherry-picking evidence and piecing it together in a way that allows them to present an arrow to the jury that appears to be an objective path to their side’s Point B.



Time for the “I’m not criticizing real-life attorneys” disclaimer!

The thing about real-world attorneys is that in an actual courtroom, the attorney way of thinking makes sense—because the attorney’s case is only half of what will be presented to the jury. The opposing attorney presents an opposing arrow that leads to the opposite Point B. This completes the picture, allowing the jury to decide which of the two arrows is more legit-seeming and which Point B seems more like the truth. In this way, the court process sets up a miniature marketplace of ideas, where opposing ideas can clash and truth will (hopefully) be left standing when the dust settles.

That’s why my many criticisms of the thinking Attorney on our ladder aren’t criticisms of actual, real-world attorneys. Real-world attorneys know they’re one half of a two-attorney system, and they know that the best way for that system to yield truth is for them to make the best possible case they can for one side of the story.

The problem for you when you’re thinking like an Attorney is that you’re not doing so as half of a complete picture, for the purpose of playing your role in a truth-finding process—you’re doing so as a thinker so flawed that winning arguments has become more important to you than truth. While a two-attorney courtroom is an excellent truth-finding mechanism, a courtroom with only one attorney in it is awful at truth—and when you’re thinking like an Attorney, your head is the latter kind of courtroom.

Forming a hypothesis, Attorney style

Before any learning has begun, your starting point looks nothing like it did when you were thinking like a Scientist. With the Higher Mind’s voice now marginalized, his humble “I don’t know” is barely audible with the Primitive Mind’s more prominent “of course I know” resonating in the center of your mind.

And as you work your way through the learning process, you treat your existing beliefs not like a revisable experiment, or even a favorite sports team, but like your client. The Scientist is the boss of their thinking process—but now, you’re working for your beliefs.

Working for your beliefs means you’re not reasoning objectively and you’re not even Reasoning While Motivated—it’s more serious than that. You’re Reasoning While Obligated.

When you’re Reasoning While Obligated, the three characters that make up your reasoning process are like law associates working on your case whose only job is to help build the case that will keep you at the Point B you started on—and, ideally, strengthen your conviction about it.

Your Attention Bouncer has an updated set of instructions: only import perspectives, statistics, anecdotes, and opinions that help confirm Point B. Regardless of how cherry season is actually going, you want to see one thing: a basket full of bright, ripe cherries. Your Attention Bouncer no longer needs to put in much thought or effort to supply you with information—you’ve made his life easy.

Attorney-style cherry-picking explains why there are so many situations where opposing sides of an argument can, simultaneously, be absolutely positive they’re right. When you’re low enough on the How You Think ladder, you stop being aware of the fact that cherry-picking is even a thing, so to you, it seems like clearly, all the evidence shows that I’m right. We see this everywhere. Like, for example, two ideologically opposed media platforms presenting the same exact news story but picking totally different cherries to present, so the parties presented as heroes and villains, victims and perpetrators, are literally reversed in their tellings.

There are plenty of less-charged examples as well. Want to believe that coffee, wine, chocolate, saturated fat, or red meat are healthy? Just type into Google the name of that item, along with “surprisingly healthy.” Cherry-picking has you covered. Want to believe they’re all terrible for you? Ask Google if they’re “harmful” or “unhealthy” to eat. Either way, you’ll come out of your search even more sure of what you came in wanting to believe. 

Your Attention Bouncer is also less intent on collecting first principles puzzle pieces than he was on the higher rungs and more content to collect pre-packaged arguments in the form of op-eds and other external opinions. When you’re looking for confirmation first and foremost, nothing is more efficient than information in a “here is why you are right” format.

Your Belief Bouncer’s new assignment is just as easy. When assessing which info is valid, instead of the super difficult job of checking IDs for truth, he now only has to check IDs for content.

When a piece of imported info jibes with what you already believe, the bouncer opens the door wide.



Info that suggests you might be wrong is seen as malicious and manipulated and consistently shut out by a skepticism filter so tight nothing could ever get through it. We have a word for that: denial.



When your Belief Bouncer judges info sources, his job is just as straightforward. When you think you already know the truth, then by definition, someone who agrees with you is right and someone who doesn’t is wrong. Instead of asking, “How did they get to this idea?” your Belief Bouncer now simply asks: “Are they enlightened?”

In such rapid “bias motion” yourself, anyone who disagrees with you, objective or biased, appears to you to be a terrible thinker. A surefire sign that you’re thinking like an Attorney is when you believe—really believe, in your heart—that the people who disagree with you are not just wrong, but wrong because of who they are—fundamentally bad thinkers—despite having a relatively small amount of experience getting to know them.

When you’re thinking like an Attorney and Reasoning While Obligated, your two-bouncer immune system is totally disabled. Your beliefs end up filled with a combination of real information and Monopoly-money junk—and you no longer have the ability to tell one from the other.

The good news is, it’s not important. Knowing what’s true and what’s not only matters when you’re actually trying to get to the truth—and your goal has now shifted to confirmation. So in the center of your mind, your Puzzler gets to work. The preferred conclusion at the periphery of the Sports Fan’s consciousness has now been moved front and center, and your Puzzler uses it as a guide, like the image on the box cover of a jigsaw puzzle.



Where Scientists have to painstakingly paint, Attorneys can mindlessly trace.

Often, the main activity for your Puzzler is simple memorization. Rather than focus on raw first principles, the bouncers have imported mostly second-hand arguments by others, and memorizing them will feel to you like gaining knowledge. You simply adopt as your own viewpoints the most legit-sounding arguments made by others who agree with you, and you’re good to go.

When you do decide to dig in a bit more and form some of your own conclusions, your Attorney process has you covered with all kinds of clever tricks involving trends and anecdotes, correlation and causation, sneakily worded statistics, and more (we’ll get into all of that in later chapters).

If someone really wants to believe just about anything—that the Earth is flat, that 9/11 was orchestrated by Americans, that everyone hates them, that everyone loves them, that the CIA is after them—the human brain will have no problem using the large toolbox of Attorney tricks to make that belief seem perfectly clear and irrefutable.

When you’re thinking like an Attorney, the Hypothesis Formation stage is really just a belief-strengthening process. You inevitably end up with the same viewpoints you started with, now beefed up with a refreshed set of facts and arguments that remind you just how right you are. You’ve constructed an arrow that does the trick.



Testing your hypothesis, Attorney style

In the testing phase, though you’re far less eager to seek out dissent to challenge your beliefs, your manufactured arrow to Point B has you ready to argue with anyone who tries to crack your conviction. You’re ready for the testing phase, because you know you’re invincible.

Your argument has nothing to worry about, because in order for dissent to generate doubt, you have to thoughtfully listen to and consider the dissent—and you won’t do either. For every argument that comes your way, you’ll listen only enough to pick out the best argument against it from your arsenal. Usually you’ll simply be reciting the words of one of the opinion pieces you imported. If someone does come at you with an argument you can’t seem to beat, you’ll again reach into your arsenal of dirty tricks.

Your refusal to really listen to or consider anything a dissenter says, compounded with your bag of trump card tricks, and topped off with your unbreakable conviction that you’re right, will ensure that you’re an absolutely infuriating person to argue with. Your opponents will feel like they’re arguing against a brick wall, and by the end, it’ll be clear to them that nothing they could have said—nothing whatsoever—could have made you say “hmm that’s a good point—I need to think about that—maybe I’m wrong.” That’s what it feels like to argue with someone who lives in Unfalsifiable Land.

The Scientist watches their ideas box as an objective spectator. The Sports Fan watches with a rooting interest. The Attorney watches boxing matches from the middle of the ring, as the corrupt ref who has fixed the match’s outcome from the moment it begun.

Let’s bring back our scientific method of knowledge acquisition for a second.



High-rung thinkers go through this arduous process because they know that knowledge is hard. When you’re thinking like an Attorney, this entire process is a farce—a formality to appease the faint voice of the Higher Mind bellowing from the backburner of your mind. Your knowledge process never had a chance to change your mind, and it never had a chance to build much real knowledge in your head.

It only took you along this path:



But the crazy thing about humans is, when you’re thinking like an Attorney, you still believe you’re thinking like a Scientist. You think you did this:



You’re sure that the immense conviction you feel has been well-earned. You think your mind is full of original viewpoints based on real, hard knowledge. You come out of those fixed-match arguments believing that you crushed it. If the argument was frustrating, you probably attribute it, ironically, to your opponent being a brick wall to argue with, and the kind of person who just can’t admit when they’re wrong.8

This is the power of human delusion. A delusional feat of this magnitude—to leave you filled with a level of conviction you have absolutely no ground to feel—is usually a tag-team effort between two types of delusion:

1) A distorted view of yourself. When you look in the mirror, instead of seeing a dramatically flawed thinker, the Primitive Mind’s thick smoke shows you a model intellectual.



2) A distorted view of the world. When you’re clear-headed and thinking like a Scientist, you’re well aware that both the world and the people in it are impossibly complex and nuanced and messy. This fact keeps you humble about what you know, no matter how much you’ve learned. But when you’re thinking like an Attorney, the same consciousness-clouding fog that lowers your self-awareness also distorts your vision of the world around you. While the Scientist’s clear vision shows them a complex, foggy world, the Attorney’s foggy vision shows them a world that’s straightforward, full of crisp lines and black-and-white distinctions. As your intellect works its way down the ladder, fuzzy spectrums sharpen into clean, binary distinctions, and unique individual people sort themselves into easily stereotyped groups. Issues now have a right side and a wrong side, with little middle ground. People are right or wrong, well-intentioned or malicious, and that’s that. Oversimplification is the amateur thinker’s trademark.

Arrogance is ignorance plus conviction. This is an especially deadly combo because it prevents you from improving. It not only leaves you without real knowledge, it deprives you of the humility needed to gain real knowledge or grow into a better thinker. When you think you’re already doing great, you feel like there’s no room left for improvement. We all collect life experience, but we don’t all take advantage of it. While humility is a permeable filter that absorbs life experience and converts it into knowledge and wisdom, arrogance is a rubber shield that life experience simply bounces off of.

___________

If there’s anything you can say about Rung 3, Attorney-style thinkers, they at least understand the concept of an arrow. They’re unfalsifiable, but they’re not that big an internal shift—an intellectual “growth spurt”—away from becoming a legitimate thinker. From somewhere in the periphery of their mind, the voice of the Higher Mind still carries some weight. And if they can just learn to listen to it and value it, maybe they can change.

But again, we’re working on a spectrum here, so the Higher Mind’s prominence in an Attorney’s head varies. The How You Think ladder works kind of like the U.S. congress. It’s a binary situation where whichever “party” has the majority is in the driver’s seat. At any point along the spectrum, either the Higher Mind or the Primitive Mind holds primary control, all determined by whether you’re below or above the ladder’s midpoint. But like congress, the margin of victory matters. When you’re thinking like an Attorney, the Primitive Mind is the majority party—but the Higher Mind is a sizable minority who still asserts influence. But as your intellect drops further down the Psych Spectrum, deeper into Unfalsifiable Land, the Higher Mind’s voice grows fainter. The Primitive Mind starts to develop a super-majority, eventually leaving the minority Higher Mind party with little to no influence.

As you descend, you become an increasingly corrupt Attorney, increasingly likely to accept red-painted plastic cherries—fake news, shoddy statistics, scattered anecdotal personal experiences—as hard evidence and universal truth. You start taking things out of context or even flat-out lying when you argue with people. And less exposure to those who disagree with you means it becomes easier to fictionalize your opponents as people not even worth talking to, making you even more certain that everything you believe is correct. At the same time, your self-image and the conviction you hold about your beliefs grow even stronger than they were. You no longer concern yourself with the reasoning behind your viewpoints—you just know that those viewpoints are right.

Eventually, your Values Stack looks like this:



With confirmation having reached the sacred section of the stack, your nail is back in the picture—while truth now swings freely.



Your thinking is now entirely running on the ancient software of your Primitive Mind, with the rational, reasonable, humble, and self-aware Higher Mind completely out of the picture.

You’ve reached the bottom rung.

Rung 4: Thinking Like a Zealot



We all think like Zealots at times. We’re all naive in our own unique set of ways, and zealot-like thinking is sometimes a case of naivety. Sometimes we’re taught zealot-like thinking by our parents or friends. Sometimes we’re scared and zealotry is a cave we’ve found to hide in.

I think it’s often because we’ve made the amateur error of thinking that people are ideas and ideas are people. If ideas and people are the same, it ties our self-worth to the worth of our beliefs. It ties our personal safety to the protection of those beliefs, and it makes a challenge to those beliefs feel like physical danger. It makes validation of our beliefs feel like acceptance and approval and love. This is one of the many pits we fall into when our Primitive Minds are doing our thinking.

When you forget that people and ideas are separate, your entire thinking process is laden with a crippling burden: to protect your beliefs like you protect your body. You’ve traded in the Scientist’s horizontal flexibility for a total willingness to vertically compromise on your thinking process, and you’ve swapped the Scientist’s vertical rigidity that keeps their process on the top rung for a horizontal rigidity that nails your feet right into the Idea Spectrum, right at the point where your viewpoints live.



No longer is there a humble Point A. When you’re thinking like a Zealot, humility feels weak and shameful. You’ll never say “I don’t know,” because that sounds the same to you as saying “I don’t know who I am.” You do know. Your beliefs are a rock-solid reflection of the objective truth, period. Knowledge is the opposite of hard—it’s like knowing the sky is blue. Anyone with a mind and a heart knows what’s true and what’s not.

So you don’t need any “process of thinking” arrow. Arrows are for idiots. This is what thinking like a Zealot feels like.

In reality, you’ve taken a set of ideas to be sacred, and you’ve given up an independent truth-seeking path in order to faithfully serve those sacred ideas. Without any Higher Mind influence forcing you to engage with some kind of knowledge process, the once-rigorous scientific method has evolved to this:



Thinking, for you, is about worship, not learning.

So aside from a deep pleasure you derive from information that praises and confirms What You Think, gathering information has little use—you already know all your cherries are perfect, so your Attention Bouncer servant can stay home.

Your Belief Bouncer doesn’t have much to do either. Ideas that validate your ideas are good, true ideas, spoken by good, reasonable people. And those are the only ideas you want to hear.

The three characters in your reasoning process who used to help you find truth now simply send in a never-ending stream of love, acceptance, approval, and safety in the form of anecdotes, statistics, and opinions that make you feel great.



In the testing ideas phase—well, there is no testing ideas phase. First of all, why would you test what you’re already sure about? It’s like testing whether the ocean is wet. But more importantly, when you think ideas and people are the same, someone challenging your ideas feels like an insult. It feels violating. It feels personally invalidating. It’s a threat. If Scientists see dissent as a puzzle piece and Sports Fans see dissent as a tennis ball, Zealots see dissent as a bucket of shit.



And who wants to challenge your ideas anyway? Dissenters. Dissenters are people who hold different ideas, which means they’re different kinds of people—worse kinds of people. Their entire existence, if accepted, invalidates your own existence. So they cannot be tolerated. You avoid dissenters and their disgusting ideas as much as you can—other than the time you spend mocking them and their ideas, which is just another form of self-confirmation.

At the top of our ladder, your identity was that of a humble learner, which made you intellectually robust. So intellectual boxing matches were your friend—they made you smarter and brought you a little closer to the truth. Now that you’re thinking like a Zealot, those ideas in the ring are your naked, vulnerable body—so you don’t want to watch your ideas box, or root for them, or even fix the matches. You ban boxing altogether.

When you’re thinking like a Zealot, there is no reasoning process, because there is no reasoning. You’re just here, always:



You’re the picture of arrogance, of unearned conviction, of total ignorance, of utter un-self-awareness. And hiding just beneath that façade is a terrifying frailty, protected only by a rigid, brittle set of simplified beliefs. It’s not a great situation.

So there’s our ladder. Time for a big recap chart:



Each of us is a work in progress, and as we grow in age, we also can grow up psychologically. The more we evolve psychologically, the more time we spend thinking from the high rungs and the less time we spend down below. But no matter how good we get at thinking, I’m pretty sure we never totally rid ourselves of low-rung thinking.

This post focused entirely on individual thinking, but the intellectual life of an individual doesn’t happen in isolation. The 2D picture we’ve painted so far in this post is still an incomplete picture. It’s a 2D cross section of what’s actually a 3D system—a single slice of bread in the loaf of human behavior. To bring our lens into full focus, we’ll need to zoom out and look deeper into the loaf, exploring how thinking happens in communities. In the next chapter, we’ll step into the wonderful, horrible world of intellectual cultures.

